{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as keras_backend\n",
    "import numpy as np\n",
    "\n",
    "gpu_on = True\n",
    "\n",
    "if gpu_on :\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    for device in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    gpu_devices = tf.config.experimental.list_physical_devices(\"CPU\")\n",
    "\n",
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "\n",
    "print(gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data num 8608 with 2 classes\n",
      "Total data num 1609 with 2 classes\n",
      "Total data num 1469 with 2 classes\n",
      "train_num: 8608\n",
      "valid_num: 1609\n",
      "test_num: 1469\n"
     ]
    }
   ],
   "source": [
    "from src.data_loader.classification import ClassifyDataloader\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from glob import glob\n",
    "\n",
    "task = \"classification\"\n",
    "data_set_name = \"detect_lvi\"\n",
    "batch_size = 32\n",
    "on_memory = True\n",
    "target_size = (512,512)\n",
    "interpolation = \"bilinear\"\n",
    "class_mode = \"binary\"\n",
    "# class_mode = \"categorical\"\n",
    "dtype=\"float32\"\n",
    "\n",
    "\n",
    "train_image_path_list = glob(f\"./datasets/{task}/{data_set_name}/train/*/*\")\n",
    "valid_image_path_list = glob(f\"./datasets/{task}/{data_set_name}/valid/*/*\")\n",
    "test_image_path_list = glob(f\"./datasets/{task}/{data_set_name}/test/*/*\")\n",
    "label_list = os.listdir(f\"./datasets/{task}/{data_set_name}/train\")\n",
    "\n",
    "label_to_index_dict = {label:index for index, label in enumerate(label_list)}\n",
    "index_to_label_dict = {index:label for index, label in enumerate(label_list)}\n",
    "\n",
    "train_data_loader = ClassifyDataloader(image_path_list=train_image_path_list,\n",
    "                                       label_to_index_dict=label_to_index_dict,\n",
    "                                       batch_size=batch_size,\n",
    "                                       on_memory=on_memory,\n",
    "                                       preprocess_input=preprocess_input,\n",
    "                                       target_size=target_size,\n",
    "                                       interpolation=interpolation,\n",
    "                                       shuffle=True,\n",
    "                                       class_mode=class_mode,\n",
    "                                       dtype=dtype\n",
    ")\n",
    "valid_data_loader = ClassifyDataloader(image_path_list=valid_image_path_list,\n",
    "                                       label_to_index_dict=label_to_index_dict,\n",
    "                                       batch_size=batch_size,\n",
    "                                       on_memory=False,\n",
    "                                       preprocess_input=preprocess_input,\n",
    "                                       target_size=target_size,\n",
    "                                       interpolation=interpolation,                                       \n",
    "                                       shuffle=False,\n",
    "                                       class_mode=class_mode,\n",
    "                                       dtype=dtype\n",
    ")\n",
    "test_data_loader = ClassifyDataloader(image_path_list=test_image_path_list,\n",
    "                                       label_to_index_dict=label_to_index_dict,\n",
    "                                       batch_size=batch_size,\n",
    "                                       on_memory=False,\n",
    "                                       preprocess_input=preprocess_input,\n",
    "                                       target_size=target_size,\n",
    "                                       interpolation=interpolation,                                      \n",
    "                                       shuffle=False,\n",
    "                                       class_mode=class_mode,\n",
    "                                       dtype=dtype\n",
    ")\n",
    "\n",
    "print(f\"train_num: {len(train_image_path_list)}\")\n",
    "print(f\"valid_num: {len(valid_image_path_list)}\")\n",
    "print(f\"test_num: {len(test_image_path_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "DROPOUT_RATIO = 0.5\n",
    "\n",
    "grad_cam = False\n",
    "transfer_learning = False\n",
    "epoch_release_frozen = 10\n",
    "transfer_train_mode = \"include_deep_layer\"\n",
    "layer_name_frozen_to = \"mixed4\"\n",
    "\n",
    "#  binary_sigmoid, categorical_sigmoid, categorical_softmax\n",
    "activation = \"binary_sigmoid\"\n",
    "\n",
    "# create the base pre-trained model~\n",
    "base_model = InceptionV3(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=(None,None,3),\n",
    "    classes=None,\n",
    "    pooling=None,\n",
    "    classifier_activation=None\n",
    ")\n",
    "\n",
    "if transfer_learning:\n",
    "    if train_mode == \"dense_only\":\n",
    "        base_model.trainable = False\n",
    "    elif train_mode == \"include_deep_layer\":\n",
    "        for layer in base_model.layers: \n",
    "            layer.trainable = False\n",
    "            if layer.name == layer_name_frozen_to:\n",
    "                break\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "# (Batch_Size,?)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(DROPOUT_RATIO)(x)\n",
    "# let's add a fully-connected layer\n",
    "# (Batch_Size,1)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# (Batch_Size,1024)\n",
    "x = Dropout(DROPOUT_RATIO)(x)\n",
    "\n",
    "if grad_cam:\n",
    "    x *= 1e-1\n",
    "    keras_backend.set_floatx('float64')\n",
    "    dense_dtype = \"float64\"\n",
    "else:\n",
    "    dense_dtype = \"float32\"\n",
    "    \n",
    "if activation == \"binary_sigmoid\":\n",
    "    predictions = Dense(1, activation='sigmoid', dtype=dense_dtype)(x)\n",
    "    loss_function = BinaryCrossentropy(label_smoothing=0.01)\n",
    "elif activation == \"categorical_sigmoid\":\n",
    "    predictions = Dense(2, activation='sigmoid', dtype=dense_dtype)(x)\n",
    "    loss_function = CategoricalCrossentropy(label_smoothing=0.01)\n",
    "elif activation == \"categorical_softmax\":\n",
    "    predictions = Dense(2, activation='softmax', dtype=dense_dtype)(x)\n",
    "    loss_function = CategoricalCrossentropy(label_smoothing=0.01)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(base_model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# YY/MM/dd\n",
    "today_str = today.strftime(\"%Y-%m-%d\")\n",
    "today_weight_path = f\"./weights/{task}/{data_set_name}/{today_str}/\" \n",
    "today_logs_path = f\"./logs/{task}/{data_set_name}/{today_str}/\"\n",
    "os.makedirs(today_weight_path, exist_ok=True)\n",
    "os.makedirs(today_logs_path, exist_ok=True)\n",
    "optimizer = Nadam(1e-3, clipnorm=1)\n",
    "\n",
    "save_c = ModelCheckpoint(\n",
    "    today_weight_path+\"/weights_{epoch:02d}_{loss:.4f}.hdf5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min')\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=5,\n",
    "    min_lr=1e-7)\n",
    "csv_logger = CSVLogger(f'{today_logs_path}/log.csv', append=False, separator=',')\n",
    "\n",
    "\n",
    "def make_model_trainable(epoch, model, target_epoch):\n",
    "    if epoch > target_epoch:\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "            \n",
    "make_trainable_callback = LambdaCallback(\n",
    "    on_epoch_begin=lambda epoch,logs: make_model_trainable(epoch, model, target_epoch=epoch_release_frozen)\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "269/269 [==============================] - 385s 1s/step - loss: 0.6694 - accuracy: 0.6281 - val_loss: 2.4328 - val_accuracy: 0.5644\n",
      "Epoch 2/200\n",
      "269/269 [==============================] - 313s 1s/step - loss: 0.4121 - accuracy: 0.8247 - val_loss: 0.4353 - val_accuracy: 0.8325\n",
      "Epoch 3/200\n",
      "269/269 [==============================] - 325s 1s/step - loss: 0.2760 - accuracy: 0.8951 - val_loss: 0.5572 - val_accuracy: 0.8125\n",
      "Epoch 4/200\n",
      "269/269 [==============================] - 329s 1s/step - loss: 0.2064 - accuracy: 0.9302 - val_loss: 0.4846 - val_accuracy: 0.8544\n",
      "Epoch 5/200\n",
      "269/269 [==============================] - 345s 1s/step - loss: 0.1550 - accuracy: 0.9507 - val_loss: 1.0723 - val_accuracy: 0.7088\n",
      "Epoch 6/200\n",
      "269/269 [==============================] - 306s 1s/step - loss: 0.1363 - accuracy: 0.9577 - val_loss: 0.4477 - val_accuracy: 0.8944\n",
      "Epoch 7/200\n",
      "269/269 [==============================] - 306s 1s/step - loss: 0.1237 - accuracy: 0.9662 - val_loss: 1.7178 - val_accuracy: 0.7719\n",
      "Epoch 8/200\n",
      "269/269 [==============================] - 310s 1s/step - loss: 0.1020 - accuracy: 0.9749 - val_loss: 0.6364 - val_accuracy: 0.7975\n",
      "Epoch 9/200\n",
      "269/269 [==============================] - 306s 1s/step - loss: 0.0899 - accuracy: 0.9783 - val_loss: 0.7320 - val_accuracy: 0.7794\n",
      "Epoch 10/200\n",
      "269/269 [==============================] - 306s 1s/step - loss: 0.0821 - accuracy: 0.9818 - val_loss: 0.4236 - val_accuracy: 0.9125\n",
      "Epoch 11/200\n",
      " 87/269 [========>.....................] - ETA: 2:47 - loss: 0.0652 - accuracy: 0.9903"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "epochs = 200\n",
    "\n",
    "model.fit(\n",
    "    train_data_loader,\n",
    "    validation_data=valid_data_loader,\n",
    "    epochs=epochs,\n",
    "    callbacks=[reduceLROnPlat, save_c, csv_logger, make_trainable_callback],\n",
    "    initial_epoch=start_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data num 1469 with 2 classes\n"
     ]
    }
   ],
   "source": [
    "test_data_loader = ClassifyDataloader(image_path_list=test_image_path_list,\n",
    "                                       label_to_index_dict=label_to_index_dict,\n",
    "                                       batch_size=batch_size,\n",
    "                                       on_memory=True,\n",
    "                                       preprocess_input=preprocess_input,\n",
    "                                       target_size=target_size,\n",
    "                                       interpolation=interpolation,                                      \n",
    "                                       shuffle=False,\n",
    "                                       class_mode=class_mode,\n",
    "                                       dtype=dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 72s 265ms/step - loss: nan - accuracy: 0.5795\n"
     ]
    }
   ],
   "source": [
    "temp = model.evaluate(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/51 [================>.............] - ETA: 34s - loss: nan - accuracy: 0.9580"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-229ea940b844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp = model.evaluate(valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_loader.class_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 0,\n",
       " 7: 0,\n",
       " 8: 0,\n",
       " 9: 0,\n",
       " 10: 0,\n",
       " 11: 0,\n",
       " 12: 0,\n",
       " 13: 0,\n",
       " 14: 0,\n",
       " 15: 0,\n",
       " 16: 0,\n",
       " 17: 0,\n",
       " 18: 0,\n",
       " 19: 0,\n",
       " 20: 0,\n",
       " 21: 0,\n",
       " 22: 0,\n",
       " 23: 0,\n",
       " 24: 0,\n",
       " 25: 0,\n",
       " 26: 0,\n",
       " 27: 0,\n",
       " 28: 0,\n",
       " 29: 0,\n",
       " 30: 0,\n",
       " 31: 0,\n",
       " 32: 0,\n",
       " 33: 0,\n",
       " 34: 0,\n",
       " 35: 0,\n",
       " 36: 0,\n",
       " 37: 0,\n",
       " 38: 0,\n",
       " 39: 0,\n",
       " 40: 0,\n",
       " 41: 0,\n",
       " 42: 0,\n",
       " 43: 0,\n",
       " 44: 0,\n",
       " 45: 0,\n",
       " 46: 0,\n",
       " 47: 0,\n",
       " 48: 0,\n",
       " 49: 0,\n",
       " 50: 0,\n",
       " 51: 0,\n",
       " 52: 0,\n",
       " 53: 0,\n",
       " 54: 0,\n",
       " 55: 0,\n",
       " 56: 0,\n",
       " 57: 0,\n",
       " 58: 0,\n",
       " 59: 0,\n",
       " 60: 0,\n",
       " 61: 0,\n",
       " 62: 0,\n",
       " 63: 0,\n",
       " 64: 0,\n",
       " 65: 0,\n",
       " 66: 0,\n",
       " 67: 0,\n",
       " 68: 0,\n",
       " 69: 0,\n",
       " 70: 0,\n",
       " 71: 0,\n",
       " 72: 0,\n",
       " 73: 0,\n",
       " 74: 0,\n",
       " 75: 0,\n",
       " 76: 0,\n",
       " 77: 0,\n",
       " 78: 0,\n",
       " 79: 0,\n",
       " 80: 0,\n",
       " 81: 0,\n",
       " 82: 0,\n",
       " 83: 0,\n",
       " 84: 0,\n",
       " 85: 0,\n",
       " 86: 0,\n",
       " 87: 0,\n",
       " 88: 0,\n",
       " 89: 0,\n",
       " 90: 0,\n",
       " 91: 0,\n",
       " 92: 0,\n",
       " 93: 0,\n",
       " 94: 0,\n",
       " 95: 0,\n",
       " 96: 0,\n",
       " 97: 0,\n",
       " 98: 0,\n",
       " 99: 0,\n",
       " 100: 0,\n",
       " 101: 0,\n",
       " 102: 0,\n",
       " 103: 0,\n",
       " 104: 0,\n",
       " 105: 0,\n",
       " 106: 0,\n",
       " 107: 0,\n",
       " 108: 0,\n",
       " 109: 0,\n",
       " 110: 0,\n",
       " 111: 0,\n",
       " 112: 0,\n",
       " 113: 0,\n",
       " 114: 0,\n",
       " 115: 0,\n",
       " 116: 0,\n",
       " 117: 0,\n",
       " 118: 0,\n",
       " 119: 0,\n",
       " 120: 0,\n",
       " 121: 0,\n",
       " 122: 0,\n",
       " 123: 0,\n",
       " 124: 0,\n",
       " 125: 0,\n",
       " 126: 0,\n",
       " 127: 0,\n",
       " 128: 0,\n",
       " 129: 0,\n",
       " 130: 0,\n",
       " 131: 0,\n",
       " 132: 0,\n",
       " 133: 0,\n",
       " 134: 0,\n",
       " 135: 0,\n",
       " 136: 0,\n",
       " 137: 0,\n",
       " 138: 0,\n",
       " 139: 0,\n",
       " 140: 0,\n",
       " 141: 0,\n",
       " 142: 0,\n",
       " 143: 0,\n",
       " 144: 0,\n",
       " 145: 0,\n",
       " 146: 0,\n",
       " 147: 0,\n",
       " 148: 0,\n",
       " 149: 0,\n",
       " 150: 0,\n",
       " 151: 0,\n",
       " 152: 0,\n",
       " 153: 0,\n",
       " 154: 0,\n",
       " 155: 0,\n",
       " 156: 0,\n",
       " 157: 0,\n",
       " 158: 0,\n",
       " 159: 0,\n",
       " 160: 0,\n",
       " 161: 0,\n",
       " 162: 0,\n",
       " 163: 0,\n",
       " 164: 0,\n",
       " 165: 0,\n",
       " 166: 0,\n",
       " 167: 0,\n",
       " 168: 0,\n",
       " 169: 0,\n",
       " 170: 0,\n",
       " 171: 0,\n",
       " 172: 0,\n",
       " 173: 0,\n",
       " 174: 0,\n",
       " 175: 0,\n",
       " 176: 0,\n",
       " 177: 0,\n",
       " 178: 0,\n",
       " 179: 0,\n",
       " 180: 0,\n",
       " 181: 0,\n",
       " 182: 0,\n",
       " 183: 0,\n",
       " 184: 0,\n",
       " 185: 0,\n",
       " 186: 0,\n",
       " 187: 0,\n",
       " 188: 0,\n",
       " 189: 0,\n",
       " 190: 0,\n",
       " 191: 0,\n",
       " 192: 0,\n",
       " 193: 0,\n",
       " 194: 0,\n",
       " 195: 0,\n",
       " 196: 0,\n",
       " 197: 0,\n",
       " 198: 0,\n",
       " 199: 0,\n",
       " 200: 0,\n",
       " 201: 0,\n",
       " 202: 0,\n",
       " 203: 0,\n",
       " 204: 0,\n",
       " 205: 0,\n",
       " 206: 0,\n",
       " 207: 0,\n",
       " 208: 0,\n",
       " 209: 0,\n",
       " 210: 0,\n",
       " 211: 0,\n",
       " 212: 0,\n",
       " 213: 0,\n",
       " 214: 0,\n",
       " 215: 0,\n",
       " 216: 0,\n",
       " 217: 0,\n",
       " 218: 0,\n",
       " 219: 0,\n",
       " 220: 0,\n",
       " 221: 0,\n",
       " 222: 0,\n",
       " 223: 0,\n",
       " 224: 0,\n",
       " 225: 0,\n",
       " 226: 0,\n",
       " 227: 0,\n",
       " 228: 0,\n",
       " 229: 0,\n",
       " 230: 0,\n",
       " 231: 0,\n",
       " 232: 0,\n",
       " 233: 0,\n",
       " 234: 0,\n",
       " 235: 0,\n",
       " 236: 0,\n",
       " 237: 0,\n",
       " 238: 0,\n",
       " 239: 0,\n",
       " 240: 0,\n",
       " 241: 0,\n",
       " 242: 0,\n",
       " 243: 0,\n",
       " 244: 0,\n",
       " 245: 0,\n",
       " 246: 0,\n",
       " 247: 0,\n",
       " 248: 0,\n",
       " 249: 0,\n",
       " 250: 0,\n",
       " 251: 0,\n",
       " 252: 0,\n",
       " 253: 0,\n",
       " 254: 0,\n",
       " 255: 0,\n",
       " 256: 0,\n",
       " 257: 0,\n",
       " 258: 0,\n",
       " 259: 0,\n",
       " 260: 0,\n",
       " 261: 0,\n",
       " 262: 0,\n",
       " 263: 0,\n",
       " 264: 0,\n",
       " 265: 0,\n",
       " 266: 0,\n",
       " 267: 0,\n",
       " 268: 0,\n",
       " 269: 0,\n",
       " 270: 0,\n",
       " 271: 0,\n",
       " 272: 0,\n",
       " 273: 0,\n",
       " 274: 0,\n",
       " 275: 0,\n",
       " 276: 0,\n",
       " 277: 0,\n",
       " 278: 0,\n",
       " 279: 0,\n",
       " 280: 0,\n",
       " 281: 0,\n",
       " 282: 0,\n",
       " 283: 0,\n",
       " 284: 0,\n",
       " 285: 0,\n",
       " 286: 0,\n",
       " 287: 0,\n",
       " 288: 0,\n",
       " 289: 0,\n",
       " 290: 0,\n",
       " 291: 0,\n",
       " 292: 0,\n",
       " 293: 0,\n",
       " 294: 0,\n",
       " 295: 0,\n",
       " 296: 0,\n",
       " 297: 0,\n",
       " 298: 0,\n",
       " 299: 0,\n",
       " 300: 0,\n",
       " 301: 0,\n",
       " 302: 0,\n",
       " 303: 0,\n",
       " 304: 0,\n",
       " 305: 0,\n",
       " 306: 0,\n",
       " 307: 0,\n",
       " 308: 0,\n",
       " 309: 0,\n",
       " 310: 0,\n",
       " 311: 0,\n",
       " 312: 0,\n",
       " 313: 0,\n",
       " 314: 0,\n",
       " 315: 0,\n",
       " 316: 0,\n",
       " 317: 0,\n",
       " 318: 0,\n",
       " 319: 0,\n",
       " 320: 0,\n",
       " 321: 0,\n",
       " 322: 0,\n",
       " 323: 0,\n",
       " 324: 0,\n",
       " 325: 0,\n",
       " 326: 0,\n",
       " 327: 0,\n",
       " 328: 0,\n",
       " 329: 0,\n",
       " 330: 0,\n",
       " 331: 0,\n",
       " 332: 0,\n",
       " 333: 0,\n",
       " 334: 0,\n",
       " 335: 0,\n",
       " 336: 0,\n",
       " 337: 0,\n",
       " 338: 0,\n",
       " 339: 0,\n",
       " 340: 0,\n",
       " 341: 0,\n",
       " 342: 0,\n",
       " 343: 0,\n",
       " 344: 0,\n",
       " 345: 0,\n",
       " 346: 0,\n",
       " 347: 0,\n",
       " 348: 0,\n",
       " 349: 0,\n",
       " 350: 0,\n",
       " 351: 0,\n",
       " 352: 0,\n",
       " 353: 0,\n",
       " 354: 0,\n",
       " 355: 0,\n",
       " 356: 0,\n",
       " 357: 0,\n",
       " 358: 0,\n",
       " 359: 0,\n",
       " 360: 0,\n",
       " 361: 0,\n",
       " 362: 0,\n",
       " 363: 0,\n",
       " 364: 0,\n",
       " 365: 0,\n",
       " 366: 0,\n",
       " 367: 0,\n",
       " 368: 0,\n",
       " 369: 0,\n",
       " 370: 0,\n",
       " 371: 0,\n",
       " 372: 0,\n",
       " 373: 0,\n",
       " 374: 0,\n",
       " 375: 0,\n",
       " 376: 0,\n",
       " 377: 0,\n",
       " 378: 0,\n",
       " 379: 0,\n",
       " 380: 0,\n",
       " 381: 0,\n",
       " 382: 0,\n",
       " 383: 0,\n",
       " 384: 0,\n",
       " 385: 0,\n",
       " 386: 0,\n",
       " 387: 0,\n",
       " 388: 0,\n",
       " 389: 0,\n",
       " 390: 0,\n",
       " 391: 0,\n",
       " 392: 0,\n",
       " 393: 0,\n",
       " 394: 0,\n",
       " 395: 0,\n",
       " 396: 0,\n",
       " 397: 0,\n",
       " 398: 0,\n",
       " 399: 0,\n",
       " 400: 0,\n",
       " 401: 0,\n",
       " 402: 0,\n",
       " 403: 0,\n",
       " 404: 0,\n",
       " 405: 0,\n",
       " 406: 0,\n",
       " 407: 0,\n",
       " 408: 0,\n",
       " 409: 0,\n",
       " 410: 0,\n",
       " 411: 0,\n",
       " 412: 0,\n",
       " 413: 0,\n",
       " 414: 0,\n",
       " 415: 0,\n",
       " 416: 0,\n",
       " 417: 0,\n",
       " 418: 0,\n",
       " 419: 0,\n",
       " 420: 0,\n",
       " 421: 0,\n",
       " 422: 0,\n",
       " 423: 0,\n",
       " 424: 0,\n",
       " 425: 0,\n",
       " 426: 0,\n",
       " 427: 0,\n",
       " 428: 0,\n",
       " 429: 0,\n",
       " 430: 0,\n",
       " 431: 0,\n",
       " 432: 0,\n",
       " 433: 0,\n",
       " 434: 0,\n",
       " 435: 0,\n",
       " 436: 0,\n",
       " 437: 0,\n",
       " 438: 0,\n",
       " 439: 0,\n",
       " 440: 0,\n",
       " 441: 0,\n",
       " 442: 0,\n",
       " 443: 0,\n",
       " 444: 0,\n",
       " 445: 0,\n",
       " 446: 0,\n",
       " 447: 0,\n",
       " 448: 0,\n",
       " 449: 0,\n",
       " 450: 0,\n",
       " 451: 0,\n",
       " 452: 0,\n",
       " 453: 0,\n",
       " 454: 0,\n",
       " 455: 0,\n",
       " 456: 0,\n",
       " 457: 0,\n",
       " 458: 0,\n",
       " 459: 0,\n",
       " 460: 0,\n",
       " 461: 0,\n",
       " 462: 0,\n",
       " 463: 0,\n",
       " 464: 0,\n",
       " 465: 0,\n",
       " 466: 0,\n",
       " 467: 0,\n",
       " 468: 0,\n",
       " 469: 0,\n",
       " 470: 0,\n",
       " 471: 0,\n",
       " 472: 0,\n",
       " 473: 0,\n",
       " 474: 0,\n",
       " 475: 0,\n",
       " 476: 0,\n",
       " 477: 0,\n",
       " 478: 0,\n",
       " 479: 0,\n",
       " 480: 0,\n",
       " 481: 0,\n",
       " 482: 0,\n",
       " 483: 0,\n",
       " 484: 0,\n",
       " 485: 0,\n",
       " 486: 0,\n",
       " 487: 0,\n",
       " 488: 0,\n",
       " 489: 0,\n",
       " 490: 0,\n",
       " 491: 0,\n",
       " 492: 0,\n",
       " 493: 0,\n",
       " 494: 0,\n",
       " 495: 0,\n",
       " 496: 0,\n",
       " 497: 0,\n",
       " 498: 0,\n",
       " 499: 0,\n",
       " 500: 0,\n",
       " 501: 0,\n",
       " 502: 0,\n",
       " 503: 0,\n",
       " 504: 0,\n",
       " 505: 0,\n",
       " 506: 0,\n",
       " 507: 0,\n",
       " 508: 0,\n",
       " 509: 0,\n",
       " 510: 0,\n",
       " 511: 0,\n",
       " 512: 0,\n",
       " 513: 0,\n",
       " 514: 0,\n",
       " 515: 0,\n",
       " 516: 0,\n",
       " 517: 0,\n",
       " 518: 0,\n",
       " 519: 0,\n",
       " 520: 0,\n",
       " 521: 0,\n",
       " 522: 0,\n",
       " 523: 0,\n",
       " 524: 0,\n",
       " 525: 0,\n",
       " 526: 0,\n",
       " 527: 0,\n",
       " 528: 0,\n",
       " 529: 0,\n",
       " 530: 0,\n",
       " 531: 0,\n",
       " 532: 0,\n",
       " 533: 0,\n",
       " 534: 0,\n",
       " 535: 0,\n",
       " 536: 0,\n",
       " 537: 0,\n",
       " 538: 0,\n",
       " 539: 0,\n",
       " 540: 0,\n",
       " 541: 0,\n",
       " 542: 0,\n",
       " 543: 0,\n",
       " 544: 0,\n",
       " 545: 0,\n",
       " 546: 0,\n",
       " 547: 0,\n",
       " 548: 0,\n",
       " 549: 0,\n",
       " 550: 0,\n",
       " 551: 0,\n",
       " 552: 0,\n",
       " 553: 0,\n",
       " 554: 0,\n",
       " 555: 0,\n",
       " 556: 0,\n",
       " 557: 0,\n",
       " 558: 0,\n",
       " 559: 0,\n",
       " 560: 0,\n",
       " 561: 0,\n",
       " 562: 0,\n",
       " 563: 0,\n",
       " 564: 0,\n",
       " 565: 0,\n",
       " 566: 0,\n",
       " 567: 0,\n",
       " 568: 0,\n",
       " 569: 0,\n",
       " 570: 0,\n",
       " 571: 0,\n",
       " 572: 0,\n",
       " 573: 0,\n",
       " 574: 0,\n",
       " 575: 0,\n",
       " 576: 0,\n",
       " 577: 0,\n",
       " 578: 0,\n",
       " 579: 0,\n",
       " 580: 0,\n",
       " 581: 0,\n",
       " 582: 0,\n",
       " 583: 0,\n",
       " 584: 0,\n",
       " 585: 0,\n",
       " 586: 0,\n",
       " 587: 0,\n",
       " 588: 0,\n",
       " 589: 0,\n",
       " 590: 0,\n",
       " 591: 0,\n",
       " 592: 0,\n",
       " 593: 0,\n",
       " 594: 0,\n",
       " 595: 0,\n",
       " 596: 0,\n",
       " 597: 0,\n",
       " 598: 0,\n",
       " 599: 0,\n",
       " 600: 0,\n",
       " 601: 0,\n",
       " 602: 0,\n",
       " 603: 0,\n",
       " 604: 0,\n",
       " 605: 0,\n",
       " 606: 0,\n",
       " 607: 0,\n",
       " 608: 0,\n",
       " 609: 0,\n",
       " 610: 0,\n",
       " 611: 0,\n",
       " 612: 0,\n",
       " 613: 0,\n",
       " 614: 0,\n",
       " 615: 0,\n",
       " 616: 0,\n",
       " 617: 0,\n",
       " 618: 0,\n",
       " 619: 0,\n",
       " 620: 0,\n",
       " 621: 0,\n",
       " 622: 0,\n",
       " 623: 0,\n",
       " 624: 0,\n",
       " 625: 0,\n",
       " 626: 0,\n",
       " 627: 0,\n",
       " 628: 0,\n",
       " 629: 0,\n",
       " 630: 0,\n",
       " 631: 0,\n",
       " 632: 0,\n",
       " 633: 0,\n",
       " 634: 0,\n",
       " 635: 0,\n",
       " 636: 0,\n",
       " 637: 0,\n",
       " 638: 0,\n",
       " 639: 0,\n",
       " 640: 0,\n",
       " 641: 0,\n",
       " 642: 0,\n",
       " 643: 0,\n",
       " 644: 0,\n",
       " 645: 0,\n",
       " 646: 0,\n",
       " 647: 0,\n",
       " 648: 0,\n",
       " 649: 0,\n",
       " 650: 0,\n",
       " 651: 0,\n",
       " 652: 0,\n",
       " 653: 0,\n",
       " 654: 0,\n",
       " 655: 0,\n",
       " 656: 0,\n",
       " 657: 0,\n",
       " 658: 0,\n",
       " 659: 0,\n",
       " 660: 0,\n",
       " 661: 0,\n",
       " 662: 0,\n",
       " 663: 0,\n",
       " 664: 0,\n",
       " 665: 0,\n",
       " 666: 0,\n",
       " 667: 0,\n",
       " 668: 0,\n",
       " 669: 0,\n",
       " 670: 0,\n",
       " 671: 0,\n",
       " 672: 0,\n",
       " 673: 0,\n",
       " 674: 0,\n",
       " 675: 0,\n",
       " 676: 0,\n",
       " 677: 0,\n",
       " 678: 0,\n",
       " 679: 0,\n",
       " 680: 0,\n",
       " 681: 0,\n",
       " 682: 0,\n",
       " 683: 0,\n",
       " 684: 0,\n",
       " 685: 0,\n",
       " 686: 0,\n",
       " 687: 0,\n",
       " 688: 0,\n",
       " 689: 0,\n",
       " 690: 0,\n",
       " 691: 0,\n",
       " 692: 0,\n",
       " 693: 0,\n",
       " 694: 0,\n",
       " 695: 0,\n",
       " 696: 0,\n",
       " 697: 0,\n",
       " 698: 0,\n",
       " 699: 0,\n",
       " 700: 0,\n",
       " 701: 0,\n",
       " 702: 0,\n",
       " 703: 0,\n",
       " 704: 0,\n",
       " 705: 0,\n",
       " 706: 0,\n",
       " 707: 0,\n",
       " 708: 0,\n",
       " 709: 0,\n",
       " 710: 0,\n",
       " 711: 0,\n",
       " 712: 0,\n",
       " 713: 0,\n",
       " 714: 0,\n",
       " 715: 0,\n",
       " 716: 0,\n",
       " 717: 0,\n",
       " 718: 0,\n",
       " 719: 0,\n",
       " 720: 0,\n",
       " 721: 0,\n",
       " 722: 0,\n",
       " 723: 0,\n",
       " 724: 0,\n",
       " 725: 0,\n",
       " 726: 0,\n",
       " 727: 0,\n",
       " 728: 0,\n",
       " 729: 0,\n",
       " 730: 0,\n",
       " 731: 0,\n",
       " 732: 0,\n",
       " 733: 0,\n",
       " 734: 0,\n",
       " 735: 0,\n",
       " 736: 0,\n",
       " 737: 0,\n",
       " 738: 0,\n",
       " 739: 0,\n",
       " 740: 0,\n",
       " 741: 0,\n",
       " 742: 0,\n",
       " 743: 0,\n",
       " 744: 0,\n",
       " 745: 0,\n",
       " 746: 0,\n",
       " 747: 0,\n",
       " 748: 0,\n",
       " 749: 0,\n",
       " 750: 0,\n",
       " 751: 0,\n",
       " 752: 0,\n",
       " 753: 0,\n",
       " 754: 0,\n",
       " 755: 0,\n",
       " 756: 0,\n",
       " 757: 0,\n",
       " 758: 0,\n",
       " 759: 0,\n",
       " 760: 0,\n",
       " 761: 0,\n",
       " 762: 0,\n",
       " 763: 0,\n",
       " 764: 0,\n",
       " 765: 0,\n",
       " 766: 0,\n",
       " 767: 0,\n",
       " 768: 0,\n",
       " 769: 0,\n",
       " 770: 0,\n",
       " 771: 0,\n",
       " 772: 0,\n",
       " 773: 0,\n",
       " 774: 0,\n",
       " 775: 0,\n",
       " 776: 0,\n",
       " 777: 0,\n",
       " 778: 0,\n",
       " 779: 0,\n",
       " 780: 0,\n",
       " 781: 0,\n",
       " 782: 0,\n",
       " 783: 0,\n",
       " 784: 0,\n",
       " 785: 0,\n",
       " 786: 0,\n",
       " 787: 0,\n",
       " 788: 0,\n",
       " 789: 0,\n",
       " 790: 0,\n",
       " 791: 0,\n",
       " 792: 0,\n",
       " 793: 0,\n",
       " 794: 0,\n",
       " 795: 0,\n",
       " 796: 0,\n",
       " 797: 0,\n",
       " 798: 0,\n",
       " 799: 0,\n",
       " 800: 0,\n",
       " 801: 0,\n",
       " 802: 0,\n",
       " 803: 0,\n",
       " 804: 0,\n",
       " 805: 0,\n",
       " 806: 0,\n",
       " 807: 0,\n",
       " 808: 0,\n",
       " 809: 0,\n",
       " 810: 0,\n",
       " 811: 0,\n",
       " 812: 0,\n",
       " 813: 0,\n",
       " 814: 0,\n",
       " 815: 0,\n",
       " 816: 0,\n",
       " 817: 0,\n",
       " 818: 0,\n",
       " 819: 0,\n",
       " 820: 0,\n",
       " 821: 0,\n",
       " 822: 0,\n",
       " 823: 0,\n",
       " 824: 0,\n",
       " 825: 0,\n",
       " 826: 0,\n",
       " 827: 0,\n",
       " 828: 0,\n",
       " 829: 0,\n",
       " 830: 0,\n",
       " 831: 0,\n",
       " 832: 0,\n",
       " 833: 0,\n",
       " 834: 0,\n",
       " 835: 0,\n",
       " 836: 0,\n",
       " 837: 0,\n",
       " 838: 0,\n",
       " 839: 0,\n",
       " 840: 0,\n",
       " 841: 0,\n",
       " 842: 0,\n",
       " 843: 0,\n",
       " 844: 0,\n",
       " 845: 0,\n",
       " 846: 0,\n",
       " 847: 0,\n",
       " 848: 0,\n",
       " 849: 0,\n",
       " 850: 0,\n",
       " 851: 0,\n",
       " 852: 0,\n",
       " 853: 0,\n",
       " 854: 0,\n",
       " 855: 0,\n",
       " 856: 0,\n",
       " 857: 0,\n",
       " 858: 0,\n",
       " 859: 0,\n",
       " 860: 0,\n",
       " 861: 0,\n",
       " 862: 0,\n",
       " 863: 0,\n",
       " 864: 0,\n",
       " 865: 0,\n",
       " 866: 0,\n",
       " 867: 0,\n",
       " 868: 0,\n",
       " 869: 0,\n",
       " 870: 0,\n",
       " 871: 0,\n",
       " 872: 0,\n",
       " 873: 0,\n",
       " 874: 0,\n",
       " 875: 0,\n",
       " 876: 0,\n",
       " 877: 0,\n",
       " 878: 0,\n",
       " 879: 0,\n",
       " 880: 0,\n",
       " 881: 0,\n",
       " 882: 0,\n",
       " 883: 0,\n",
       " 884: 0,\n",
       " 885: 0,\n",
       " 886: 0,\n",
       " 887: 0,\n",
       " 888: 0,\n",
       " 889: 1,\n",
       " 890: 1,\n",
       " 891: 1,\n",
       " 892: 1,\n",
       " 893: 1,\n",
       " 894: 1,\n",
       " 895: 1,\n",
       " 896: 1,\n",
       " 897: 1,\n",
       " 898: 1,\n",
       " 899: 1,\n",
       " 900: 1,\n",
       " 901: 1,\n",
       " 902: 1,\n",
       " 903: 1,\n",
       " 904: 1,\n",
       " 905: 1,\n",
       " 906: 1,\n",
       " 907: 1,\n",
       " 908: 1,\n",
       " 909: 1,\n",
       " 910: 1,\n",
       " 911: 1,\n",
       " 912: 1,\n",
       " 913: 1,\n",
       " 914: 1,\n",
       " 915: 1,\n",
       " 916: 1,\n",
       " 917: 1,\n",
       " 918: 1,\n",
       " 919: 1,\n",
       " 920: 1,\n",
       " 921: 1,\n",
       " 922: 1,\n",
       " 923: 1,\n",
       " 924: 1,\n",
       " 925: 1,\n",
       " 926: 1,\n",
       " 927: 1,\n",
       " 928: 1,\n",
       " 929: 1,\n",
       " 930: 1,\n",
       " 931: 1,\n",
       " 932: 1,\n",
       " 933: 1,\n",
       " 934: 1,\n",
       " 935: 1,\n",
       " 936: 1,\n",
       " 937: 1,\n",
       " 938: 1,\n",
       " 939: 1,\n",
       " 940: 1,\n",
       " 941: 1,\n",
       " 942: 1,\n",
       " 943: 1,\n",
       " 944: 1,\n",
       " 945: 1,\n",
       " 946: 1,\n",
       " 947: 1,\n",
       " 948: 1,\n",
       " 949: 1,\n",
       " 950: 1,\n",
       " 951: 1,\n",
       " 952: 1,\n",
       " 953: 1,\n",
       " 954: 1,\n",
       " 955: 1,\n",
       " 956: 1,\n",
       " 957: 1,\n",
       " 958: 1,\n",
       " 959: 1,\n",
       " 960: 1,\n",
       " 961: 1,\n",
       " 962: 1,\n",
       " 963: 1,\n",
       " 964: 1,\n",
       " 965: 1,\n",
       " 966: 1,\n",
       " 967: 1,\n",
       " 968: 1,\n",
       " 969: 1,\n",
       " 970: 1,\n",
       " 971: 1,\n",
       " 972: 1,\n",
       " 973: 1,\n",
       " 974: 1,\n",
       " 975: 1,\n",
       " 976: 1,\n",
       " 977: 1,\n",
       " 978: 1,\n",
       " 979: 1,\n",
       " 980: 1,\n",
       " 981: 1,\n",
       " 982: 1,\n",
       " 983: 1,\n",
       " 984: 1,\n",
       " 985: 1,\n",
       " 986: 1,\n",
       " 987: 1,\n",
       " 988: 1,\n",
       " 989: 1,\n",
       " 990: 1,\n",
       " 991: 1,\n",
       " 992: 1,\n",
       " 993: 1,\n",
       " 994: 1,\n",
       " 995: 1,\n",
       " 996: 1,\n",
       " 997: 1,\n",
       " 998: 1,\n",
       " 999: 1,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_loader.data_getter.class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(list(valid_data_loader.data_getter.class_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
